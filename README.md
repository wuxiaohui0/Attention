# Attention
这里主要存放在实验中使用到的不同的Attention实现方法，来源于github中其他的博主，只是在这里记录一下方便使用的时候找起来比较方便！！！！


ACmix:来源于论文On the Integration of Self-Attention and Convolution.
概述：主要是讲来充分利用CNN和Attention的优点来提高效率。
